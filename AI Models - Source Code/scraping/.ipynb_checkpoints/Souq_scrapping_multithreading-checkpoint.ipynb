{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Souq Products\n",
    "\n",
    "**Welcome to this scrapping Notebook, what in this notebook:**\n",
    "\n",
    "- Implement helpful functions that help us scrap some images related to different categories because of our image classification model, which depends on multiple different categories that will help us in future work to recommend you images related to your style.\n",
    "\n",
    "**Structure of the Scrapping I used:**\n",
    "- loading packages we need.\n",
    "- handle some firefox preference and options that help us during process of scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading packages we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from souq_configs import *\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "import os\n",
    "from time import sleep\n",
    "import re\n",
    "import sys\n",
    "from time import sleep\n",
    "from multiprocessing import Process\n",
    "from selenium.webdriver.common.by import By\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path the work dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    current_path = os.path.dirname(os.path.abspath(__file__))\n",
    "except:\n",
    "    current_path = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle some firefox preference and options that help us during process of scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_driver(gecko_driver='', user_agent='', load_images=True, is_headless=False):\n",
    "    '''\n",
    "        This function is just to set up some of default for browser\n",
    "    '''\n",
    "    firefox_profile = webdriver.FirefoxProfile()\n",
    "    \n",
    "    firefox_profile.set_preference('dom.ipc.plugins.enabled.libflashplayer.so', False)\n",
    "    firefox_profile.set_preference(\"media.volume_scale\", \"0.0\")\n",
    "    firefox_profile.set_preference(\"dom.webnotifications.enabled\", False)\n",
    "    if user_agent != '':\n",
    "        firefox_profile.set_preference(\"general.useragent.override\", user_agent)\n",
    "    if not load_images:\n",
    "        firefox_profile.set_preference('permissions.default.image', 2)\n",
    "\n",
    "    options = Options()\n",
    "    options.add_argument('headless')\n",
    "    options.headless = is_headless\n",
    "    \n",
    "    driver = webdriver.Firefox(options=options,\n",
    "                               executable_path=f'{current_path}/{gecko_driver}',\n",
    "                               firefox_profile=firefox_profile)\n",
    "    \n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(url, driver):\n",
    "    '''\n",
    "    Argument:\n",
    "        url of any page to get\n",
    "        driver that was finalized and what we will use as robot in the opened pages\n",
    "    return:\n",
    "        True\n",
    "    '''\n",
    "    driver.get(url)\n",
    "    driver.refresh()\n",
    "    sleep(2)\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function explanation\n",
    "\n",
    "The driver send as paramter is a one page and for each page we send as driver we get all of the products:\n",
    "- Loop over these products\n",
    "- Extract image src\n",
    "- Extract image name and extension using regular expression\n",
    "- Save images based on the type in different directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def products_info(driver, image_type):\n",
    "    '''\n",
    "    Argumetn:\n",
    "        Driver of the page we are in\n",
    "    return:\n",
    "        save images in its direction based on its type\n",
    "    '''\n",
    "\n",
    "    products = driver.find_elements_by_css_selector(\"div.tpl-results div.grid-list div.single-item\")\n",
    "    \n",
    "    for product in products:\n",
    "        # take a based selector\n",
    "        selector = product.find_elements_by_css_selector\n",
    "        image_src = ''\n",
    "        try:\n",
    "            image_src = selector('a.img-link img')\n",
    "            image_src = image_src[0].get_attribute('data-src')\n",
    "            # check image_src has actuall src or None\n",
    "            if image_src:\n",
    "                img_name_ext = re.search('\\w+(.jpg)', image_src)\n",
    "                if image_type == 't_shirts':\n",
    "                    urllib.request.urlretrieve(image_src, \"images/t_shirts/\"+img_name_ext[0])\n",
    "                elif image_type == 'shirts':\n",
    "                    urllib.request.urlretrieve(image_src, \"images/shirts/\"+img_name_ext[0])\n",
    "                elif image_type == 'pants':\n",
    "                    urllib.request.urlretrieve(image_src, \"images/pants/\"+img_name_ext[0])\n",
    "        # handle errors generated of any reasons\n",
    "        except Exception as e:\n",
    "            file = open(\"logs_files/products_info.log\",\"+a\")\n",
    "            file.write(\"This error related to function products_info of Souq_scrapping_multithreading file\\n\" \n",
    "               + str(e) + \"\\n\" + \"#\" *99 + \"\\n\") # \"#\" *99 as separated lines\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function\n",
    "\n",
    "**This function work as follow:**\n",
    "- loop over all pages url\n",
    "- get driver for each page then after scraping all images of this page quit the page\n",
    "- check if there is new page or not to reset the process\n",
    "- for each page call product info function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_pages(page_url, image_type, next_page = 1):\n",
    "    '''\n",
    "    Argument:\n",
    "        next page = 1 as default value\n",
    "        page_url to as start page\n",
    "    return:\n",
    "        dictionary for all pages contain:\n",
    "        for each page get all prdocuts info contain:\n",
    "        for each prodcut get all reviews and main features  \n",
    "    '''\n",
    "    all_page_products = {}\n",
    "    while next_page:\n",
    "# get the driver first\n",
    "        url = page_url + str(next_page)\n",
    "        driver = init_driver(gecko_driver,user_agent=user_agent)\n",
    "        _ = get_url(url, driver)\n",
    "# get page products info and for each product get all features and reviews\n",
    "        products_infos = products_info(driver)\n",
    "        all_page_products[str(next_page)] = products_info\n",
    "# check for new pages\n",
    "        showMore = driver.find_element_by_css_selector('.pagination-next a')\n",
    "        next_page_url = showMore.get_attribute('href')\n",
    "        _ = get_url(next_page_url, driver)\n",
    "        next_page +=1\n",
    "        current_url = driver.current_url\n",
    "        driver.close()\n",
    "# get the page current page number\n",
    "        current_url = re.findall('page=[0-9]+', current_url)\n",
    "        current_url = re.findall('[0-9]+', str(current_url))\n",
    "        current_url = \"\".join(current_url)\n",
    "        if current_url != str(next_page):\n",
    "            next_page = 1\n",
    "        driver.quit()\n",
    "    return all_page_products\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    p1 = Process(target=scrap_pages, args=(souq__section_url_tishrt, 't_shirts', 1))\n",
    "    p1.start()\n",
    "    p2 = Process(target=scrap_pages, args=(souq__section_url_shirts, 'shirts', 1))\n",
    "    p2.start()\n",
    "    p3 = Process(target=scrap_pages, args=(souq__section_url_pants, 'pants', 1))\n",
    "    p3.start()\n",
    "    p1.join()\n",
    "    p2.join()\n",
    "    p3.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
